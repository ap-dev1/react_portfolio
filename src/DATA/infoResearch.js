
//import placeholder from '../assets/placeholder.png'

var infoResearch = [

    {
        name: "intro",
        level: 1,
        content: "My work focuses on the dynamics of continuous decision-making. I approach this question from a complexity theory perspective, using biologically-inspired computational methods and human laboratory studies. In my most recent work 1) I explored the effects of spontaneous variation on the behavior variability of virtual agents and the mediating role of the environment’s value (Popa et. al., 2015) and structure (Popa, 2015a), 2) I showed how computational approaches can inform about the emergence of developmental disorders such as ADHD and how the complexity paradigm can inform on real-world issues (Popa & McDowell, 2015), and 3) I developed a human-friendly version of the continuous-choice procedure used in non-humans that yielded robust results (“good matching”) in under 30 minutes, thus making it possible to directly investigate the neuronal underpinnings of continuous decision-making (Popa, 2015b).",
    },



    {
        name: "Behavioral repertoires as Complex Adaptive Systems",
        level: 1,
        content: "One of the more successful accounts (for an overview, see McDowell, 2013b) was inspired by the functional analogy between learning and biological evolution (Pringle, 1951). The basic idea is straightforward: it proposes that, much like the phenotypes in a biopopulation, behavioral repertoires are shaped by low-level processes of selection and variation. These processes act locally, here-and-now, and allow organisms to adapt in dynamic environments. Over time, they produce high-level behavioral structures and properties that cannot be predicted by examining the local processes that produced them. These high-level outcomes are called emergent and systems that exhibit emergence are referred to as complex systems (Holland, 1998). The dynamics of complex systems can be studied computationally, as discussed in McDowell and Popa (2009): implement its parts and processes in an abstract form, allow the system to evolve, and compare the outcomes against those observed in biological organisms.",
    },


    {
        name: "Computational Darwinism 1",
        level: 1,
        content: "The computational model discussed here (McDowell, 2004) implements a population of behaviors, positive reinforcement, and low-level rules of selection, recombination, and mutation. Behaviors have phenotypes and genotypes, represented by integers (e.g. 42) and their corresponding binary form (e.g., 101010). Behaviors can be separated into target classes, functionally equivalent to the physical levers used in operant chambers. Target behaviors, on occasions, can acquire reinforcers; non-target behaviors are never reinforced.",
    },


    {
        name: "Computational Darwinism 2",
        level: 1,
        content: "An experiment begins with a random population (generation 1), equivalent to a naïve, untrained organism; from it, one behavior is chosen at random and emitted. The emission, equivalent to one lever press, triggers the Darwinian cycle, which transforms the existing population into a new one (generation 2). From it, one behavior is emitted, a new population is created (generation 3), and so on. Reinforcing events tend to concentrate the next generation around the most recent, just-reinforced emission. Non-reinforced emissions result in more dispersed, variable populations. The emissions are recorded and analyzed as if they were the behaviors of a biological organism.",
    },

    {
        name: "PRE-CANDIDACY WORK",
        level: 1,
        content: "",
    },

    {
        name: "Agreement with mathematical descriptors of steady-state behavior",
        level: 2,
        content: "The field of quantitative analysis of behavior generated a wealth of fine-grained knowledge about operant behavior. This includes the “matching equations”, known to describe the overall relation between responses and reinforcers with great accuracy, and across species (for a review, see McDowell, 2013a). These equations were used to assess the behavior generated by the computational model. In McDowell and Popa (2010) we showed that when reinforcement rate varied (constant reinforcer magnitude), choice behavior was described in quantitative detail by Baum’s (1974) power function matching equation. In McDowell, Popa, and Calvin (2012) we showed that when both reinforcement rate and magnitude varied, choice behavior was well described by the bivariate (or generalized) matching equation (Baum & Rachlin, 1969; reviewed by Cording, McLean, & Grace, 2011). In both cases, the equations explained most of the variance preference patterns (pVAF > 98%) and yielded parameter values for sensitivity to reinforcement rate (ar ~0.8) and magnitude (am ~0.6) indistinguishable from those observed in live organisms. These outcomes emerged, unguided, from the reiteration of Darwinian rules, providing support for the evolutionary account of behavior dynamics.",
    },

    {
        name: "Hamming Distances in ETBD and Changeover Delays in the real world",
        level: 2,
        content: "The everyday is filled with distractions that can interfere with activities that require sustained effort. One solution is to make switching between activities more difficult. A common practice is to arrange a changeover delay (COD; Findley, 1958): a short interval contingent on switching behavior, during which all reinforcers are withheld (hence delayed). This is known to decrease the frequency of changeovers and increase sensitivity to reinforcement. For my master’s project, I showed that a computational variable (Popa & McDowell, 2010; see also Popa, 2013, p. 19-21) is functionally equivalent with the changeover delay (COD) arranged in experiments with living organisms: high values produced fewer changeovers and higher sensitivity to reinforcement (and vice-versa). In this document, I denote this variable with hdCOD (Hamming COD).",
    },

    {
        name: "DISSERTATION",
        level: 1,
        content: "",
    },


    {
        name: "1. Mutation rate, environmental characteristics, and maladaptive behavioral variability",
        level: 2,
        content: "In the model, each emission, or response, initiates a Darwinian cycle. After selection and recombination create a new population, it is affected by mutation: a certain percent of behaviors, referred to as mutation rate, is randomly selected and “mutated”: one bit in its “genotype”, chosen at random, is “flipped” from 1 to 0 or 0 to 1. Previous work, some of which was presented at ABAI (2011), suggested that high mutation rates may produce behavioral characteristics that could be described as impulsive. The first specific aim of my dissertation project was to explore the effects of various mutation rates, under different environmental conditions, on variability and organization of choice behavior. <br> High mutation rates produced severely disorganized behavior, strikingly similar to those observed in ADHD-diagnosed children: “…decreased sensitivity to reward, switched more often between activities, engaged less often in continuous-responding, abandoned the task faster, took longer to re-engage, acquired fewer resources, exhibited high levels of topographic variability…” (from Popa, 2013, page 42). Similar to the behavioral symptoms observed in ADHD, the disorganized behavior produced by high mutation rates were counteracted in “good environ” and worsened in “bad” environs (p. 43-46).",
    },




    {
        name: "From mutation rate to ADHD and brain function",
        level: 3,
        content: "These findings led me to hypothesize that mutation rate may be functionally equivalent to the level of spontaneous fluctuations in the brain’s default-mode network (DMN; Raichle et. al., 2001), recently shown to be associated with high levels of intra-individual variability (Buckner et. al., 2008; Weissman et. al., 2006) and to interfere with goal-directed activity (ref). This hypothesis can be explored by recording brain activity while the participant is engaged in continuous choice-behavior. The experimental procedure would have to be short enough to be used in conjunction with brain-investigation technologies (e.g., EEG, ERP, fMRI), but long enough to produce high frequencies of responses and reinforcing events. ",
    },

    {
        name: "2. Verifying model’s predictions in a fast-paced, continuous-choice environment",
        level: 2,
        content: "For the second part of my project I programed and tested a “virtual operant chamber”. I arranged a continuous-choice environ in which five concurrent schedules, each in effect for only 200 seconds, scheduled very high reinforcement rates (e.g., 100 reinforcers/minute). One group of students experienced a 2-seconds changeover delay (COD_2s); the second group did not (No_COD). Their choice behavior was compared along the same eight behavioral dimensions used to explore behavioral variability in the computational model. <br> When a two-second COD was contingent on switching, behavioral variability noticeably lower than in the No_COD condition. This provided further support for the equivalence between COMPCOD and COD and showed that computational models can be used to predict human behavior (Popa, 2013, p. 59 - 60). Furthermore, behavior was relatively well described by the power function matching equation (COD_2s; a ~ 0.7, b ~ 1.0, pVAF ~ 75%), despite the short experimental duration and very high reinforcement rates.",
    },

    {
        name: "From computational models to brain function, via fast-paced procedures",
        level: 3,
        content: "My findings, unique at the time of their publication, suggest that fast-paced procedures can be used to explore equivalences between computational variables and properties of the nervous system. For example, if mutation rate and the level of spontaneous fluctuations in the brain’s default mode network (DMN) are functionally equivalent, then choice-behavior that correlates with high levels of DMN fluctuations should match that produced by high mutation rates, as discussed in Popa & McDowell (2016).",
    },

    {
        name: "POST-DOCTORAL WORK",
        level: 1,
        content: "",
    },

    {
        name: "Computational agents (ETBD.py)",
        level: 2,
        content: "Currently, I am developing an evolutionary model that, in addition to selection events (positive reinforcement), implements elimination events and contextual cues. The first step is to verify if the steady-state behavior it generates conforms with known mathematical descriptors. Pilot studies showed that choice-behavior in symmetrical RI RI schedules was described, in quantitative detail, by the power-function matching equation (a ~ 0.8; b ~ 1.0; pVAF > 98%). Next, I will verify if the elimination events (negative selection) act as punishers and if contextual cues become discriminative stimuli. As verification criteria, I will use existing findings, such as Bradshaw’s et al. results on the effects of response cost in single-schedules (1978) and predictions made by quantitative models of punishment (e.g., de Villiers, 1980).",
    },

    {
        name: "Next Best Thing vs Optimality: one step closer towards organic neuronal activity",
        level: 2,
        content: "xxxx.",
    },



    {
        name: "Fixed and Variable Reinforcer Magnitude",
        level: 2,
        content: "The specific effects of Fixed Interval (FI) and Variable Interval (VI) schedules of reinforcement have been investigated extensively. In all studies however, the magnitude of the reinforcer was always a fixed quantity (like time in FI schedules).  - The goal of this study is to address this gap and explore the effects of variable reinforcer magnitudes in a Python-based model of behavior dynamics (see above).  - I hypothesize that allowing reinforcement magnitude to vary from trial to trial will have similar effects as those observed when the interval to reinforcement is allowed to vary from trial to trial (VI vs. FI).  - Beyond behavior analysis and behavior economics, this work can inform basic research in neuroscience, because reinforcer magnitude (in the real world) is directly relevant to neuronal dynamics (via the summation principle).",
    },



    {
        name: "Phenotypic Density and Sensitivity to Reinforcement: Implications for Neuronal Darwinism",
        level: 2,
        content: "aim 1, completed: build a model of reinforcement learning from scratch using Python;  - aim 2, completed: test it in concurrent-schedule environments;  - aim 3: explore the effects of phenotypic density on behavioral variability and organization",
    },


    {
        name: "xxx",
        level: 2,
        content: "xxx",
    },



    {
        name: "Human agents",
        level: 1,
        content: "In parallel, I will use the human research program to expand our knowledge on the effects of discriminative stimuli and avoidance/escape behavior. Recent work, conducted in collaboration with my former students (see Vitae), has already produced results that can further inform and verify computational developments. For example, we investigated human behavior in concurrent schedules, with signaled and unsignaled (invisible) target classes (with Tang, X.). The outcomes of these experiments can be compared to behavior generated by the model in the presence or absence of contextual cues. In other studies, we explored the effects of extinction on positively reinforced behavior (with Forbes, A.) and on escape behavior (Popa & Grissom, 2017), the latter most likely unique in the human literature.",
    },



    {
        name: "SpARC 2016",
        level: 2,
        content: "xxx",
    },


    {
        name: "xxx",
        level: 2,
        content: "xxx",
    },


    {
        name: "Effects of Positive and Negative Reinforcement on Rule Learning",
        level: 2,
        content: "We explored the effects of reinforcement type on learning and its underlying neuronal activity - we recorded EEG data while participants were engaged in operant behavior. We found that participants discovered abstract rules twice as fast when correct responses resulted in gaining points (positive reinforcement) then when they resulted in not losing points (negative reinforcement);",
    },


    {
        name: "Current work and future directions",
        level: 1,
        content: "",
    },


    {
        name: "Social Interactions as “behavioral Turing tests” for the evolutionary model",
        level: 2,
        content: "Both the model and the human experimental procedure can be expanded to the study of social interactions. In the model this can be done by instantiating multiple populations of behaviors and allowing them to evolve entangled, each emission being both behavior and consequence (for others). Similarly, the human procedure can be expanded to “multiplayer mode” (so to speak). Bridging the two will allow humans to interact with other humans and/or with digital agents. Will they recognize the difference between human and computer by behavior alone? Will they cooperate? Compete? Avoid each other? Will in-groups and out-groups spontaneously emerge?",
    },


    {
        name: "Significance",
        level: 1,
        content: "The question that drives my research, how behavior changes over time, is directly relevant to all phenomena in which adaptive behavior plays a significant role, from child rearing and personality development, to global warming and artificial intelligence. My approach is inherently transdisciplinary, combining theoretical developments and experimental techniques from experimental analysis of behavior, computer science, developmental psychology, cognitive-neuroscience, evolutionary biology, and information theory, Building upon ideas from these domains, I seek to make fundamental contributions to our understanding of the basic principles that underlie moment-to-moment changes in voluntary action, cognitive processes, and their underlying neuronal activity.",
    },



    {
        id: 998,
        name: "ETBD.py - Phenotypic Density and Sensitivity to Reinforcement: Implications for Neuronal Darwinism",
        //tags: ["xxx", "xxx", "xxx"],
        content: "aim 1, completed: build a model of reinforcement learning from scratch using Python;  - aim 2, completed: test it in concurrent-schedule environments;  - aim 3: explore the effects of phenotypic density on behavioral variability and organization",
        img: [],
        urls: [
            // { name: "Demo", link: 'https://aqueous-plains-89974.herokuapp.com' },
            // { name: "GitHub", link: 'https://github.com/ap-dev1/clock_in' },
        ]
    },




]


export default infoResearch;
