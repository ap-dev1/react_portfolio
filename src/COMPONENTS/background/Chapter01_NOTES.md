


/* 

    var text1 =
"The field of quantitative analysis of behavior generated a wealth of fine-grained knowledge about operant behavior. This includes the “matching equations”, known to describe the overall relation between responses and reinforcers with great accuracy, and across species (for a review, see McDowell, 2013a). These equations were used to assess the behavior generated by ETBD. ❤ In McDowell and Popa (2010) we showed that when reinforcement rate varied (constant reinforcer magnitude), choice behavior was described in quantitative detail by Baum’s (1974) power function matching equation.  ❤ In McDowell, Popa, and Calvin (2012) we showed that when both reinforcement rate and magnitude varied, choice behavior was well described by the bivariate (or generalized) matching equation (Baum & Rachlin, 1969; reviewed by Cording, McLean, & Grace, 2011).  ❤ In Popa & McDowell (2016) we showed that the assumption of relativity that underlies matching theory did not hold for ETBD data. This was congruent with some live data and in contradictions with others, becoming a prediction of ETBD.  ❤  ❤  ❤  Note that in all studies, the equations explained most of the variance (pVAF > 98%) and yielded parameter values indistinguishable from those observed in live organisms. <span className='star'>&#9734</span> Note also that these outcomes emerged, unguided, from the reiteration of Darwinian rules, providing support for the evolutionary account of behavior dynamics.";
 */




{/* This mini-experiment arranges two sources of reinforcement - the red and green regions. Each mouse click represents a response, a choice between red and green. Over time, choices amount to <i>preference patterns</i> that can be analyzed on many dimensions. */}








{/* <h1 className="chapter-title" style={{ fontSize: "1.5rem", margin: "2rem 0rem 1rem 0rem" }}>Changeover Delays</h1>

<p className="p-left">
{parse("When two sources of reinforcement are simoultaneously available, the agent can be reinforced for responding on A, for responding on B, or for <i>changing over</i> from one to the other, which is problematic. An effective strategy for preventing the third scenario is to implement a <i>changeover delay</i>, or COD: a few seconds during which reinforcers are withheld (Findley, 1950s).")}
</p> */}











{/* <p id="ch01_p2" className="p-left">
                    For the first three years I worked exclusively with synthetic data generated by an evolutionary theory of behavior dynamics, or ETBD (McDowell, 2004). Before we get into it, let's see what it's supposed to explain.
                </p> */}




{/* <p className="p-left">{parse("For these comparisons to be meaningful, the variables and rules that make ETBD must map on the real world. <i>Selection</i>, for example was shown to be functionally equivalent to positive reinforcement, which means that numbers and ranges are also suitable representations of operant responses and operant classes, espectivelly.")}</p> */}


{/* <p className="p-left">{parse("ETBD was built to verify Skinner's analogy between learning and evolution, or, more specifically, between reinforcement and selection. Behaviors are represented by numbers and target classes by ranges. As for agents, they are represented by populations of numbers that are transformed, generation after generation, by local rules of <i>selection</i>, <i>recombination</i>, and <i>mutation</i>.")}</p> */}












{/* <p className="p-left">{parse("This mini-experiment arranges two sources of reinforcement - the red and green regions. Each mouse click represents a response, a choice between red and green. Over time, choices amount to <i>preference patterns</i> that can be analyzed on many dimensions.")}</p> */}

{/* <h2 className="chapter-sub-title">Verifying computational theories</h2>

<p className="p-left">{parse("In ETBD, behaviors are represented by numbers and target classes are represented by ranges.")}</p>


<p className="p-left">{parse("As for  agents, they are represented by populations of numbers that are transformed, generation after generation, by simple rules of <i>selection</i>, <i>recombination</i>, and <i>mutation</i>.")}</p>


<p className="p-left">{parse("The emerging patterns are then compared with those exhibited by living agents under similar experimental conditions.")}</p> */}

{/* 
<ReadMoreHook
maxChars={300}
overview={parse("The emerging patterns are then compared with those exhibited by living agents under similar experimental conditions.")}
/> */}

{/* <ReadMoreHook
maxChars={300}
overview={parse("For these comparisons to be meaningful, the variables and rules that make ETBD must map on the real world. <i>Selection</i>, for example was shown to be functionally equivalent to positive reinforcement, which means that numbers and ranges are also suitable representations of operant responses and operant classes, espectivelly.")}
/> */}